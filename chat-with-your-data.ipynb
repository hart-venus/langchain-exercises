{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chat with Your Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in this example, we'll employ the six step method of retrieval augmented generation to semantically search and conversate over document files of different types."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Library installs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in c:\\python311\\lib\\site-packages (0.0.314)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\python311\\lib\\site-packages (from langchain) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\python311\\lib\\site-packages (from langchain) (2.0.22)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\python311\\lib\\site-packages (from langchain) (3.8.6)\n",
      "Requirement already satisfied: anyio<4.0 in c:\\python311\\lib\\site-packages (from langchain) (3.7.1)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in c:\\python311\\lib\\site-packages (from langchain) (0.6.1)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\python311\\lib\\site-packages (from langchain) (1.33)\n",
      "Requirement already satisfied: langsmith<0.1.0,>=0.0.43 in c:\\python311\\lib\\site-packages (from langchain) (0.0.43)\n",
      "Requirement already satisfied: numpy<2,>=1 in c:\\users\\aleyv\\appdata\\roaming\\python\\python311\\site-packages (from langchain) (1.25.1)\n",
      "Requirement already satisfied: pydantic<3,>=1 in c:\\python311\\lib\\site-packages (from langchain) (2.4.2)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\python311\\lib\\site-packages (from langchain) (2.31.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in c:\\python311\\lib\\site-packages (from langchain) (8.2.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\python311\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in c:\\python311\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (3.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\python311\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\python311\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (4.0.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\python311\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\python311\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\python311\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\python311\\lib\\site-packages (from anyio<4.0->langchain) (3.4)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\python311\\lib\\site-packages (from anyio<4.0->langchain) (1.3.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\python311\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (3.20.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in c:\\python311\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (0.9.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\python311\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain) (2.4)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\python311\\lib\\site-packages (from pydantic<3,>=1->langchain) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.10.1 in c:\\python311\\lib\\site-packages (from pydantic<3,>=1->langchain) (2.10.1)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in c:\\python311\\lib\\site-packages (from pydantic<3,>=1->langchain) (4.8.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\python311\\lib\\site-packages (from requests<3,>=2->langchain) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\python311\\lib\\site-packages (from requests<3,>=2->langchain) (2023.7.22)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\python311\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.0)\n",
      "Requirement already satisfied: packaging>=17.0 in c:\\users\\aleyv\\appdata\\roaming\\python\\python311\\site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json<0.7,>=0.5.7->langchain) (23.2)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\python311\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (1.0.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\aleyv\\appdata\\roaming\\python\\python311\\site-packages (1.25.1)\n",
      "Requirement already satisfied: tiktoken in c:\\users\\aleyv\\appdata\\roaming\\python\\python311\\site-packages (0.5.1)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\aleyv\\appdata\\roaming\\python\\python311\\site-packages (from tiktoken) (2023.10.3)\n",
      "Requirement already satisfied: requests>=2.26.0 in c:\\python311\\lib\\site-packages (from tiktoken) (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\python311\\lib\\site-packages (from requests>=2.26.0->tiktoken) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\python311\\lib\\site-packages (from requests>=2.26.0->tiktoken) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\python311\\lib\\site-packages (from requests>=2.26.0->tiktoken) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\python311\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2023.7.22)\n",
      "Requirement already satisfied: openai in c:\\python311\\lib\\site-packages (0.28.1)\n",
      "Requirement already satisfied: requests>=2.20 in c:\\python311\\lib\\site-packages (from openai) (2.31.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\aleyv\\appdata\\roaming\\python\\python311\\site-packages (from openai) (4.65.0)\n",
      "Requirement already satisfied: aiohttp in c:\\python311\\lib\\site-packages (from openai) (3.8.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\python311\\lib\\site-packages (from requests>=2.20->openai) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\python311\\lib\\site-packages (from requests>=2.20->openai) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\python311\\lib\\site-packages (from requests>=2.20->openai) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\python311\\lib\\site-packages (from requests>=2.20->openai) (2023.7.22)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\python311\\lib\\site-packages (from aiohttp->openai) (23.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\python311\\lib\\site-packages (from aiohttp->openai) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\python311\\lib\\site-packages (from aiohttp->openai) (4.0.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\python311\\lib\\site-packages (from aiohttp->openai) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\python311\\lib\\site-packages (from aiohttp->openai) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\python311\\lib\\site-packages (from aiohttp->openai) (1.3.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\aleyv\\appdata\\roaming\\python\\python311\\site-packages (from tqdm->openai) (0.4.6)\n",
      "Requirement already satisfied: pypdf in c:\\python311\\lib\\site-packages (3.16.4)\n",
      "Requirement already satisfied: chromadb in c:\\users\\aleyv\\appdata\\roaming\\python\\python311\\site-packages (0.4.14)\n",
      "Requirement already satisfied: requests>=2.28 in c:\\python311\\lib\\site-packages (from chromadb) (2.31.0)\n",
      "Requirement already satisfied: pydantic>=1.9 in c:\\python311\\lib\\site-packages (from chromadb) (2.4.2)\n",
      "Requirement already satisfied: chroma-hnswlib==0.7.3 in c:\\users\\aleyv\\appdata\\roaming\\python\\python311\\site-packages (from chromadb) (0.7.3)\n",
      "Requirement already satisfied: fastapi>=0.95.2 in c:\\users\\aleyv\\appdata\\roaming\\python\\python311\\site-packages (from chromadb) (0.103.2)\n",
      "Requirement already satisfied: uvicorn[standard]>=0.18.3 in c:\\users\\aleyv\\appdata\\roaming\\python\\python311\\site-packages (from chromadb) (0.23.2)\n",
      "Requirement already satisfied: posthog>=2.4.0 in c:\\users\\aleyv\\appdata\\roaming\\python\\python311\\site-packages (from chromadb) (3.0.2)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in c:\\python311\\lib\\site-packages (from chromadb) (4.8.0)\n",
      "Requirement already satisfied: pulsar-client>=3.1.0 in c:\\users\\aleyv\\appdata\\roaming\\python\\python311\\site-packages (from chromadb) (3.3.0)\n",
      "Requirement already satisfied: onnxruntime>=1.14.1 in c:\\users\\aleyv\\appdata\\roaming\\python\\python311\\site-packages (from chromadb) (1.16.1)\n",
      "Requirement already satisfied: tokenizers>=0.13.2 in c:\\users\\aleyv\\appdata\\roaming\\python\\python311\\site-packages (from chromadb) (0.14.1)\n",
      "Requirement already satisfied: pypika>=0.48.9 in c:\\users\\aleyv\\appdata\\roaming\\python\\python311\\site-packages (from chromadb) (0.48.9)\n",
      "Requirement already satisfied: tqdm>=4.65.0 in c:\\users\\aleyv\\appdata\\roaming\\python\\python311\\site-packages (from chromadb) (4.65.0)\n",
      "Requirement already satisfied: overrides>=7.3.1 in c:\\users\\aleyv\\appdata\\roaming\\python\\python311\\site-packages (from chromadb) (7.4.0)\n",
      "Requirement already satisfied: importlib-resources in c:\\users\\aleyv\\appdata\\roaming\\python\\python311\\site-packages (from chromadb) (6.1.0)\n",
      "Requirement already satisfied: grpcio>=1.58.0 in c:\\users\\aleyv\\appdata\\roaming\\python\\python311\\site-packages (from chromadb) (1.59.0)\n",
      "Requirement already satisfied: bcrypt>=4.0.1 in c:\\users\\aleyv\\appdata\\roaming\\python\\python311\\site-packages (from chromadb) (4.0.1)\n",
      "Requirement already satisfied: typer>=0.9.0 in c:\\users\\aleyv\\appdata\\roaming\\python\\python311\\site-packages (from chromadb) (0.9.0)\n",
      "Requirement already satisfied: numpy>=1.22.5 in c:\\users\\aleyv\\appdata\\roaming\\python\\python311\\site-packages (from chromadb) (1.25.1)\n",
      "Requirement already satisfied: anyio<4.0.0,>=3.7.1 in c:\\python311\\lib\\site-packages (from fastapi>=0.95.2->chromadb) (3.7.1)\n",
      "Requirement already satisfied: starlette<0.28.0,>=0.27.0 in c:\\users\\aleyv\\appdata\\roaming\\python\\python311\\site-packages (from fastapi>=0.95.2->chromadb) (0.27.0)\n",
      "Requirement already satisfied: coloredlogs in c:\\users\\aleyv\\appdata\\roaming\\python\\python311\\site-packages (from onnxruntime>=1.14.1->chromadb) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in c:\\users\\aleyv\\appdata\\roaming\\python\\python311\\site-packages (from onnxruntime>=1.14.1->chromadb) (23.5.26)\n",
      "Requirement already satisfied: packaging in c:\\users\\aleyv\\appdata\\roaming\\python\\python311\\site-packages (from onnxruntime>=1.14.1->chromadb) (23.2)\n",
      "Requirement already satisfied: protobuf in c:\\users\\aleyv\\appdata\\roaming\\python\\python311\\site-packages (from onnxruntime>=1.14.1->chromadb) (4.24.4)\n",
      "Requirement already satisfied: sympy in c:\\python311\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb) (1.12)\n",
      "Requirement already satisfied: six>=1.5 in c:\\python311\\lib\\site-packages (from posthog>=2.4.0->chromadb) (1.16.0)\n",
      "Requirement already satisfied: monotonic>=1.5 in c:\\users\\aleyv\\appdata\\roaming\\python\\python311\\site-packages (from posthog>=2.4.0->chromadb) (1.6)\n",
      "Requirement already satisfied: backoff>=1.10.0 in c:\\users\\aleyv\\appdata\\roaming\\python\\python311\\site-packages (from posthog>=2.4.0->chromadb) (2.2.1)\n",
      "Requirement already satisfied: python-dateutil>2.1 in c:\\python311\\lib\\site-packages (from posthog>=2.4.0->chromadb) (2.8.2)\n",
      "Requirement already satisfied: certifi in c:\\python311\\lib\\site-packages (from pulsar-client>=3.1.0->chromadb) (2023.7.22)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\python311\\lib\\site-packages (from pydantic>=1.9->chromadb) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.10.1 in c:\\python311\\lib\\site-packages (from pydantic>=1.9->chromadb) (2.10.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\python311\\lib\\site-packages (from requests>=2.28->chromadb) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\python311\\lib\\site-packages (from requests>=2.28->chromadb) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\python311\\lib\\site-packages (from requests>=2.28->chromadb) (1.26.16)\n",
      "Requirement already satisfied: huggingface_hub<0.18,>=0.16.4 in c:\\users\\aleyv\\appdata\\roaming\\python\\python311\\site-packages (from tokenizers>=0.13.2->chromadb) (0.17.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\aleyv\\appdata\\roaming\\python\\python311\\site-packages (from tqdm>=4.65.0->chromadb) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\aleyv\\appdata\\roaming\\python\\python311\\site-packages (from typer>=0.9.0->chromadb) (8.1.6)\n",
      "Requirement already satisfied: h11>=0.8 in c:\\users\\aleyv\\appdata\\roaming\\python\\python311\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.14.0)\n",
      "Requirement already satisfied: httptools>=0.5.0 in c:\\users\\aleyv\\appdata\\roaming\\python\\python311\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.6.0)\n",
      "Requirement already satisfied: python-dotenv>=0.13 in c:\\users\\aleyv\\appdata\\roaming\\python\\python311\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.0.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\python311\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (6.0.1)\n",
      "Requirement already satisfied: watchfiles>=0.13 in c:\\users\\aleyv\\appdata\\roaming\\python\\python311\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.21.0)\n",
      "Requirement already satisfied: websockets>=10.4 in c:\\users\\aleyv\\appdata\\roaming\\python\\python311\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (11.0.3)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\python311\\lib\\site-packages (from anyio<4.0.0,>=3.7.1->fastapi>=0.95.2->chromadb) (1.3.0)\n",
      "Requirement already satisfied: filelock in c:\\python311\\lib\\site-packages (from huggingface_hub<0.18,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.12.2)\n",
      "Requirement already satisfied: fsspec in c:\\users\\aleyv\\appdata\\roaming\\python\\python311\\site-packages (from huggingface_hub<0.18,>=0.16.4->tokenizers>=0.13.2->chromadb) (2023.9.2)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in c:\\users\\aleyv\\appdata\\roaming\\python\\python311\\site-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb) (10.0)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\python311\\lib\\site-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
      "Requirement already satisfied: pyreadline3 in c:\\users\\aleyv\\appdata\\roaming\\python\\python311\\site-packages (from humanfriendly>=9.1->coloredlogs->onnxruntime>=1.14.1->chromadb) (3.4.1)\n",
      "Requirement already satisfied: lark in c:\\users\\aleyv\\appdata\\roaming\\python\\python311\\site-packages (1.1.7)\n"
     ]
    }
   ],
   "source": [
    "! pip install langchain --user\n",
    "! pip install numpy --user\n",
    "! pip install tiktoken --user\n",
    "! pip install openai --user\n",
    "! pip install pypdf --user\n",
    "! pip install chromadb --user\n",
    "! pip install lark --user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Setup openAI API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import openai \n",
    "\n",
    "# get api key from system environment variable\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Loading Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GameGPT: Multi-agent Collaborative Framework for\n",
      "Game Development\n",
      "Dake Chen\n",
      "AutoGame Research\n",
      "dk@autogame.aiHanbin Wang\n",
      "X-Institute\n",
      "wanghanbin@mails.x-institute.edu.cn\n",
      "Yunhao Huo\n",
      "University of Southern California\n",
      "hhuo@usc.eduYuzhao Li\n",
      "AutoGame Research\n",
      "ram@autogame.aiHaoyang Zhang\n",
      "AutoGame Research\n",
      "17@autogame.ai\n",
      "Abstract\n",
      "The large language model (LLM) based agents have demonstrated their capacity\n",
      "to automate and expedite software development processes. In this paper, we\n",
      "focus on game development and propose a multi-agent collaborative framework,\n",
      "dubbed GameGPT, to automate game development. While many studies have\n",
      "pinpointed hallucination as a primary roadblock for deploying LLMs in production,\n",
      "we identify another concern: redundancy. Our framework presents a series of\n",
      "methods to mitigate both concerns. These methods include dual collaboration and\n",
      "layered approaches with several in-house lexicons, to mitigate the hallucination\n",
      "and redundancy in the planning, task identification, and implementation phases.\n",
      "Furthermore, a decoupling approach is also introduced to achieve code generation\n",
      "with better precision.\n",
      "1 Introduction\n",
      "Artificial intelligence’s applications in game development can be traced back to classic games\n",
      "such as Starcraft andDiablo [1–3]. Developers have consistently required AI systems for crafting\n",
      "interactive virtual worlds and characters. These systems have become standard in the development of\n",
      "such interactive platforms. Early game AI research emphasizes controlling non-player characters\n",
      "(NPCs) and pathfinding [ 4]. With the advancement of natural language processing (NLP), some\n",
      "pioneering works that focus on generating levels using the deep learning technique have emerged.\n",
      "A representative is MarioGPT [ 5], which successfully generates levels in Super Mario Bros by\n",
      "fine-tuning GPT2 [6].\n",
      "Recently, transformer-based large language models (LLMs) have achieved substantial advancements,\n",
      "making notable strides in both natural language processing and computer vision [ 7–9,6,10–12].\n",
      "The training of LLMs is a multi-phase process. The initial phase involves training these models on\n",
      "extensive corpora, fostering the acquisition of fundamental language capabilities. In the subsequent\n",
      "phase, which is of considerable significance, the models are fine-tuned via data for a diverse of NLP\n",
      "tasks that are delineated through instructions [ 13–16]. This instruction tuning enhances the models’\n",
      "ability to generalize across a wide range of applications, leading to noteworthy zero-shot performance\n",
      "on unseen tasks. Lastly, the reinforcement learning from human feedback (RLHF) phase guarantees\n",
      "the models’ structural integrity and reliability [ 17]. More importantly, this phase also grants the\n",
      "model the capacity to generate content that emulates human style, thereby enhancing its versatility as\n",
      "an agent.\n",
      "Moreover, the advancement of LLMs has catalyzed the utilization of agents in automating software\n",
      "development processes. Various studies have explored the deployment of a single LLM-based agentarXiv:2310.08067v1  [cs.AI]  12 Oct 2023\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "def get_pdf_pages(path):\n",
    "    return PyPDFLoader(path).load()\n",
    "\n",
    "# the atomic object that all objects share\n",
    "# is Document. All pdf pages are Document objects.\n",
    "# this same process can be done for 80 different filetypes but for this example we will use pdfs\n",
    "\n",
    "print(get_pdf_pages(\"docs/arxiv.2310.08067.pdf\")[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document splitting step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GameGPT: Multi-agent Collaborative Framework for\n",
      "Game Development\n",
      "Dake Chen\n",
      "AutoGame Research\n",
      "dk@autogame.aiHanbin Wang\n",
      "X-Institute\n",
      "wanghanbin@mails.x-institute.edu.cn\n",
      "Yunhao Huo\n",
      "University of Southern California\n",
      "hhuo@usc.eduYuzhao Li\n",
      "AutoGame Research\n",
      "ram@autogame.aiHaoyang Zhang\n",
      "AutoGame Research\n",
      "17@autogame.ai\n",
      "Abstract\n",
      "The large language model (LLM) based agents have demonstrated their capacity\n",
      "to automate and expedite software development processes. In this paper, we\n",
      "{'Header 1': 'This is a markdown header'}\n",
      "{'Header 1': 'This is a markdown header', 'Header 2': 'This is a subheader'}\n"
     ]
    }
   ],
   "source": [
    "# this is an important step, as you need to keep semantically complete chunks of text together\n",
    "# to prevent incomplete sentences from being generated\n",
    "\n",
    "# there are a whole lotta document splitters for langchain, including\n",
    "# CharacterTextSplitter, MarkdownHeaderTextSplitter, TokenTextSplitter, SentenceTransformersTextSplitter, \n",
    "# RecursiveCharacterTextSplitter, Language, NLTKTextSplitter, SpacyTextSplitter, and more\n",
    "\n",
    "# for generic text, it's usually best to uzse RecursiveCharacterTextSplitter\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter, MarkdownHeaderTextSplitter\n",
    "recursive_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500, # means different things to different splitters, this is for character length in each chunk.\n",
    "    chunk_overlap=0, # how many characters are shared to keep context\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"] # if it fails to split on the first separator, it will try the next one\n",
    ")\n",
    "\n",
    "split_docs = recursive_splitter.split_documents(get_pdf_pages(\"docs/arxiv.2310.08067.pdf\"))\n",
    "print(split_docs[0].page_content)\n",
    "\n",
    "# Sometimes it's useful to use custom metadata to help the model understand the context of the document\n",
    "# some splitters, like the markdown splitter, will automatically extract metadata from the document based \n",
    "# on the markdown headers\n",
    "\n",
    "markdown_text = \"\"\"\n",
    "# This is a markdown header\n",
    "This is some text\n",
    "## This is a subheader\n",
    "This is some more text\n",
    "\"\"\"\n",
    "\n",
    "markdown_splitter = MarkdownHeaderTextSplitter(\n",
    "    headers_to_split_on=[\n",
    "        (\"#\", \"Header 1\"),\n",
    "        (\"##\", \"Header 2\"),\n",
    "        (\"###\", \"Header 3\"),\n",
    "        (\"####\", \"Header 4\"),\n",
    "        (\"#####\", \"Header 5\"),\n",
    "        (\"######\", \"Header 6\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(markdown_splitter.split_text(markdown_text)[0].metadata)\n",
    "print(markdown_splitter.split_text(markdown_text)[1].metadata) # keeps the heading 1 metadata\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector Embedding Step\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.917245426514217\n",
      "0.756001034298859\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "# Embeddings are a way to represent text as a vector of numbers\n",
    "# that semantically represent the text. This is useful for comparing\n",
    "# the similarity of two pieces of text, or for generating text that\n",
    "# is similar to the input text.\n",
    "\n",
    "embed = OpenAIEmbeddings()\n",
    "\n",
    "# dot product makes it easy to compare the similarity of two pieces of text\n",
    "import numpy as np\n",
    "dogs_embed = embed.embed_query(\"I like dogs\")\n",
    "cats_embed = embed.embed_query(\"I like cats\")\n",
    "bycicles_embed = embed.embed_query(\"Bycicles are a form of transportation\")\n",
    "\n",
    "print(np.dot(dogs_embed, cats_embed))\n",
    "print(np.dot(dogs_embed, bycicles_embed))\n",
    "\n",
    "# the first two are more similar than the second two, thus the dot product is higher\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector Store Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorstores\n",
    "# Vectorstores are a way to store embeddings for a large amount of text\n",
    "import os\n",
    "import shutil\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "path = \"docs/chroma/\"\n",
    "# remove existing vectorstore\n",
    "if os.path.exists(path):\n",
    "    shutil.rmtree(path)\n",
    "\n",
    "db = Chroma.from_documents(\n",
    "    documents=split_docs,\n",
    "    embedding=embed,\n",
    "    persist_directory=path\n",
    ")\n",
    "\n",
    "db.persist() # saves the vectorstore to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "address the limitations of the LLM and the temporal constraints of game development, we integrate\n",
      "multiple agents with distinct roles into the framework. This integration aims to enhance precision\n",
      "and scalability. The scalability aspect of GameGPT offers the potential to create games of medium to\n",
      "large sizes. Moreover, GameGPT operates in a collaborative manner, exhibiting a dual collaboration\n",
      "approach. Firstly, it involves cooperation between the LLMs and smaller expert models dedicated\n",
      "---\n",
      "tasks in game development. In order to perform the classification, a BERT model is employed to\n",
      "effectively categorize each task. The BERT model has been trained with an in-house dataset. This\n",
      "dataset contains data entries uniquely tailored to the tasks of game development. The input is a task\n",
      "drawn from the predetermined list, while the output corresponds to the task’s designated category.\n",
      "Identifying the argument involves another LLM. The agent provides a template that corresponds to the\n"
     ]
    }
   ],
   "source": [
    "# you can now query semantic similarity of the documents\n",
    "question = \"Overview of GameGPT\"\n",
    "res = db.similarity_search(question, k=1) # k is the number of results to return\n",
    "print(res[0].page_content)\n",
    "print('---')\n",
    "question = \"What is task classification?\"\n",
    "res = db.similarity_search(question, k=1)\n",
    "print(res[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieval Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(page_content='Dogs are big'), Document(page_content='Crocodiles are big')]\n",
      "[Document(page_content='Dogs are big'), Document(page_content='Ford fiestas are a type of car')]\n"
     ]
    }
   ],
   "source": [
    "# You can enforce diversity in the results by using the max_marginal_relevance_search function\n",
    "# this database will not be saved to disk\n",
    "\n",
    "minidb = Chroma.from_texts([\n",
    "    \"Rats are small\",\n",
    "    \"Dogs are big\",\n",
    "    \"Crocodiles are big\",\n",
    "    \"Cats are small\",\n",
    "    \"Ford fiestas are a type of car\",\n",
    "], embedding=embed)\n",
    "\n",
    "print(minidb.similarity_search(\"big animals\", k=2))\n",
    "\n",
    "print(minidb.max_marginal_relevance_search(\n",
    "    \"big animals\",\n",
    "    k=2, # the ones ACTUALLY selected\n",
    "    fetch_k=5, # the ones to choose from\n",
    "))\n",
    "# it will select the most diverse from the \n",
    "# fetch_k results, and return the k most diverse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'page': 3, 'source': 'docs/arxiv.2310.08067.pdf'} stages, the game engine testing engineer undertakes the execution of tasks and subsequently produces\n",
      "a comprehensive result summary.\n",
      "2.2 Multi-agent Framework\n",
      "In GameGPT, each agent maintains a private memory system and can access the shared public\n",
      "discussion to acquire the necessary information for guiding their decision-making process. For agent\n",
      "iat time step t, this process can be formally represented as follows:\n",
      "pθi(Oit|Mit, Pt), (1)\n",
      "4 tasks in game development. In order to perform the classification, a BERT model is employed to\n",
      "effectively categorize each task. The BERT model has been trained with an in-house dataset. This\n",
      "dataset contains data entries uniquely tailored to the tasks of game development. The input is a task\n",
      "drawn from the predetermined list, while the output corresponds to the task’s designated category.\n",
      "Identifying the argument involves another LLM. The agent provides a template that corresponds to the\n",
      "3 address these challenges, we present four strategies designed to alleviate these concerns. All four\n",
      "strategies are orthogonal to each other and can be layered to achieve better effectiveness.\n",
      "Game Genre Classification with Template The first strategy is to perform classification for the\n",
      "incoming request, aimed at discerning the genre intended for the game. Presently, the GameGPT\n",
      "framework accommodates development for five distinct game genres, namely, [e.g., action, strategy,\n",
      "4 to actively review, rectify, and enhance the plan in accordance with their expectations. This approach\n",
      "safeguards alignment between the devised plan and the users’ desires.\n",
      "2.4 Game Development Task Classification\n",
      "The process of task Classification within GameGPT demands a high accuracy in identifying both the\n",
      "task type and its corresponding arguments. Consequently, to ensure the accuracy of this phase, the\n",
      "2 to specific tasks, thereby enhancing the decision-making process. Secondly, collaboration occurs\n",
      "among agents assigned different roles, contributing to the decision-rectification and minimizing the\n",
      "hallucination of LLMs.\n",
      "Figure 1 provides an overview of the proposed GameGPT framework, which operates through five\n",
      "distinct stages: game development planning, task classification, code generation, task execution, and\n",
      "---\n",
      "{'page': 7, 'source': 'docs/arxiv.2310.08067.pdf'}\n"
     ]
    }
   ],
   "source": [
    "# You can filter manually by metadata\n",
    "docs = db.similarity_search(\n",
    "    \"Summary of GameGPT\",\n",
    "    k=5,\n",
    "    filter= {\n",
    "        \"page\": 3\n",
    "    }\n",
    ")\n",
    "\n",
    "print(docs[0].metadata, docs[0].page_content)\n",
    "\n",
    "# You can infer desired metadata from the query itself\n",
    "from langchain.llms import OpenAI\n",
    "import lark \n",
    "from langchain.retrievers.self_query.base import SelfQueryRetriever\n",
    "from langchain.chains.query_constructor.schema import AttributeInfo\n",
    "\n",
    "metadata_fields = [\n",
    "    AttributeInfo(\n",
    "        name=\"page\",\n",
    "        type=\"integer\",\n",
    "        description=\"the page of the document\"\n",
    "    )\n",
    "]\n",
    "\n",
    "retriever = SelfQueryRetriever.from_llm(\n",
    "    OpenAI(temperature=0),\n",
    "    db,\n",
    "    \"A scientific paper about GameGPT\",\n",
    "    metadata_fields,\n",
    "    verbose=True\n",
    ")\n",
    "# a self query retriever splits up the question asked into a query and a filter\n",
    "# filter addresses metadata, query addresses the text itself\n",
    "# query isn't exactly the question asked, but rather keywords that are extracted from the question\n",
    "\n",
    "docs = retriever.get_relevant_documents(\"First four pages, tell me about categorization\")\n",
    "\n",
    "for doc in docs:\n",
    "    print(doc.metadata[\"page\"], doc.page_content)\n",
    "\n",
    "print('---')\n",
    "\n",
    "docs = retriever.get_relevant_documents(\"seventh page\")\n",
    "\n",
    "for doc in docs:\n",
    "    print(doc.metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python311\\Lib\\site-packages\\langchain\\chains\\llm.py:280: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n",
      "c:\\Python311\\Lib\\site-packages\\langchain\\chains\\llm.py:280: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n",
      "c:\\Python311\\Lib\\site-packages\\langchain\\chains\\llm.py:280: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n",
      "c:\\Python311\\Lib\\site-packages\\langchain\\chains\\llm.py:280: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "• To address the hallucination and redundancy concerns of LLMs within game development,\n",
      "several mitigations including dual collaboration and code decoupling are proposed.\n",
      "•Empirical results demonstrate the GameGPT’s capability in effective decision-making and\n",
      "decision-rectifying throughout the game development process.\n",
      "2 GameGPT\n",
      "2.1 Overview\n",
      "The GameGPT framework is designed as a specialized multi-agent system for game development. To\n"
     ]
    }
   ],
   "source": [
    "# you can also use compression to extract the \n",
    "# most important semantic information from a document\n",
    "# and cram more documents into the same context window\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import LLMChainExtractor\n",
    "\n",
    "llm = OpenAI(temperature=0)\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=LLMChainExtractor.from_llm(llm),\n",
    "    base_retriever=db.as_retriever(search_type=\"mmr\") # optional search_type param (max marginal relevance)\n",
    ")\n",
    "\n",
    "print(compression_retriever.get_relevant_documents(\"What is GameGPT?\")[0].page_content)\n",
    "\n",
    "# there are also a whole bunch of other retrievers\n",
    "# but that is out of the scope of this example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output Generation Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GameGPT is a specialized multi-agent system designed for game development. It integrates multiple agents with distinct roles to address the limitations of language models and the time constraints of game development. GameGPT operates in a collaborative manner, involving cooperation between language models and smaller expert models. It also employs approaches such as instruction tuning, code decoupling, and dual collaboration to address hallucination and redundancy concerns. The framework aims to enhance precision, scalability, and decision-making throughout the game development process.\n",
      "GameGPT is a specialized multi-agent system for game development that integrates multiple agents with distinct roles to enhance precision and scalability. It employs a combination of approaches, including dual collaboration and code decoupling, to address hallucination and redundancy concerns. Thanks for asking!\n",
      "• To address the hallucination and redundancy concerns of LLMs within game development,\n",
      "several mitigations including dual collaboration and code decoupling are proposed.\n",
      "•Empirical results demonstrate the GameGPT’s capability in effective decision-making and\n",
      "decision-rectifying throughout the game development process.\n",
      "2 GameGPT\n",
      "2.1 Overview\n",
      "The GameGPT framework is designed as a specialized multi-agent system for game development. To\n",
      "GameGPT is a specialized multi-agent system and game development framework that integrates multiple agents with distinct roles into the game development process. It aims to address the limitations of language models and the temporal constraints of game development. GameGPT offers scalability and operates in a collaborative manner, involving cooperation between language models and smaller expert models dedicated to specific tasks. It strategically employs a combination of approaches to address hallucination and redundancy, including dual collaboration, instruction tuning through in-house lexicons, and code decoupling.\n"
     ]
    }
   ],
   "source": [
    "# Using the RetrievalQA chain \n",
    "# This api does not keep conversation history \n",
    "\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "llm = ChatOpenAI(temperature=0)\n",
    "chain = RetrievalQA.from_chain_type(\n",
    "    llm,\n",
    "    retriever=db.as_retriever()\n",
    ")\n",
    "\n",
    "print(chain({\"query\": \"What is GameGPT?\"})[\"result\"])\n",
    "\n",
    "# you can also use a prompt template to help with prompt engineering\n",
    "\n",
    "template = \"\"\"\n",
    "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. Use three sentences maximum. Keep the answer as concise as possible. Always say \"thanks for asking!\" at the end of the answer. \n",
    "{context}\n",
    "Question: {question}\n",
    "Helpful Answer:\n",
    "\"\"\"\n",
    "\n",
    "chain_prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "chain = RetrievalQA.from_chain_type(\n",
    "    llm, \n",
    "    retriever=db.as_retriever(),\n",
    "    return_source_documents=True, # returns the source documents that were used to answer the question\n",
    "    chain_type_kwargs={\n",
    "        \"prompt\": chain_prompt\n",
    "    }\n",
    ")\n",
    "\n",
    "result = chain({\"query\": \"What is GameGPT?\"})\n",
    "print(result[\"result\"])\n",
    "print(result[\"source_documents\"][0].page_content)\n",
    "\n",
    "# There are also multiple chain types you can use with this api \n",
    "\n",
    "chain_map_reduce = RetrievalQA.from_chain_type(\n",
    "    llm,\n",
    "    retriever=db.as_retriever(),\n",
    "    chain_type=\"map_reduce\",\n",
    ")\n",
    "\n",
    "# map-reduce uses an llm to reduce the documents to the most important info via LLM\n",
    "# then passes it back to generate the answer\n",
    "\n",
    "print(chain_map_reduce({\"query\": \"What is GameGPT?\"})[\"result\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory and Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One potential use case for GameGPT is in the development of video games. GameGPT can be used as a specialized multi-agent system to assist game developers in various aspects of the game development process. It can help with decision-making, decision-rectifying, and addressing concerns such as hallucination and redundancy within the game development process. By integrating multiple agents with distinct roles, GameGPT aims to enhance precision and scalability in game development. It can also classify game genres and provide guidance based on the specific genre intended for the game. Overall, GameGPT offers the potential to improve the efficiency and effectiveness of game development.\n",
      "The provided context does not explicitly mention other potential use cases for GameGPT. Therefore, it is unclear what other applications or domains GameGPT could be used for.\n"
     ]
    }
   ],
   "source": [
    "# To add memory we use ConversationBufferMemory\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "memory = ConversationBufferMemory(\n",
    "    memory_key=\"chat_history\",\n",
    "    return_messages=True \n",
    ")\n",
    "# and now, we use this chain with the conversational retrieval chain\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "qa = ConversationalRetrievalChain.from_llm(\n",
    "    llm,\n",
    "    retriever=db.as_retriever(),\n",
    "    memory=memory\n",
    ")\n",
    "\n",
    "print(qa({\"question\": \"Give me an use case for GameGPT.\"})[\"answer\"])\n",
    "print(qa({\"question\": \"Give me another one.\"})[\"answer\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
